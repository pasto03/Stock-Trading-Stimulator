v1.1是基于v1版本做出的变化。其中改变如下：

模拟环境：
- 变化：
1. 环境返回的state从当前天数变为[当前资金，当前持股数]
2. reset函数同样重置当前资产， 并返回重置后资产
3. 用户选择持仓不会出现任何惩罚了 -> reward = 0
4. 引入全新函数`compare_and_count`取代`softmin`和`softmax`

- 小结：
1. 对于买入和卖出行为的reward会直接影响模型效果
2. 计算reward的思路比实际函数实现更加重要


模型和agent：
- 变化：
1. 进行了调整以适应模拟环境返回的state


模型训练：
- 变化：
1. 定义了方法`pretrained_trading_agent`，可以在应用agent之前对其进行预训练


模型评估：
- 变化：
1. 定义了方法`agent_grid_search`，尝试找到效果最佳的num_episodes


结论：
实际上这个版本的表现比其他两个都要糟糕得多，并且稳定性也严重下滑了。
我认为原因在于，模拟环境中reward的设定。我们必须找到一个合适的reward计算方式，才能真正有效的提升agent的效果。
因此，我们接下来需要专注在找到一个有效计算reward的思路，才能进入这个模型的v2时代。


